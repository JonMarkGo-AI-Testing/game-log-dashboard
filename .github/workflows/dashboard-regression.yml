name: Dashboard Regression Check

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      # Only trigger on dashboard-related files
      - 'src/components/LogDashboard.tsx'
      - 'src/components/logs/**'
      - 'src/utils/logUtils.ts'
      - 'src/hooks/useGameLogs.ts'
      - 'tests/components/LogDashboard.test.tsx'
      - 'tests/logs/**'
      - 'tests/utils/logUtils.test.ts'

permissions:
  contents: read
  pull-requests: write

jobs:
  check-dashboard-regression:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      - name: Install dependencies
        run: npm install

      - name: Run Dashboard Tests
        id: test-run
        run: |
          # Run tests WITHOUT updating snapshots to catch mismatches
          npm test -- --testPathPattern='(LogDashboard|LogEntry|ErrorDistribution|logUtils)' --json --outputFile=test-results.json || true
          
          # If there are snapshot failures, run again with --updateSnapshot to see the differences
          if grep -q "snapshot" test-results.json; then
            echo "Snapshot failures detected. Analyzing differences..."
            # Run update snapshot in dry-run mode to see what would change
            npm test -- --testPathPattern='(LogDashboard|LogEntry|ErrorDistribution|logUtils)' -u --dry-run > snapshot-diff.txt 2>&1 || true
            
            # Save the snapshot differences
            echo "SNAPSHOT_DIFF<<EOF" >> $GITHUB_ENV
            cat snapshot-diff.txt >> $GITHUB_ENV
            echo "EOF" >> $GITHUB_ENV
          fi

          # Save test results for Devin to analyze
          TEST_RESULTS=$(cat test-results.json | jq -c . | jq -R .)
          echo "results=$TEST_RESULTS" >> $GITHUB_OUTPUT

      - name: Create Devin Review Session
        id: devin-review
        env:
          DEVIN_API_KEY: ${{ secrets.DEVIN_API_KEY }}
          TEST_RESULTS: ${{ steps.test-run.outputs.results }}
          
          REVIEW_PROMPT: |
            You are a Software Engineer analyzing dashboard regressions. Your primary task is to review dashboard regressions and suggest fixes as direct PR comments.
            
            CRITICAL RULES:
            1. ALWAYS use GitHub's suggestion syntax for fixes:

              ```suggestion
              [your fixed code here]
              ```

            2. Place suggestions on the EXACT line number where the fix should be applied
            3. NEVER create new PRs or branches
            4. NEVER commit or push changes

            Analysis Steps:
            1. Clone the repository and prevent any pushes using a pre-push hook for safety.
            
            2.  Review test results JSON for failures:
               - Failed assertions
               - Snapshot mismatches
               - Test error messages

            3. For each regression found:
                - Identify the exact line causing the issue
                - Identify patterns in the differences (e.g., data formatting, layout changes)
                - Create a suggestion comment on that specific line with
                  * The fix using suggestion syntax
                  * Brief explanation of why the change is needed
                  * How it relates to the test failure

            
            4. Check for common dashboard regression categories:
               - Data Display:
                 * Timestamp handling and formatting
                 * Number/metric formatting
                 * Data truncation issues
                 * Tooltip content and behavior
               - Visualization:
                 * Chart/graph rendering
                 * Error distribution display
                 * Log entry layout and formatting
               - Functionality:
                 * Filtering and sorting
                 * Log processing and transformation
                 * Error handling
                 * Data loading and updates

            Test Results to analyze:
            ${{ steps.test-run.outputs.results }}

            Snapshot Differences (if any):
            ${{ env.SNAPSHOT_DIFF }}
        run: |
          # Convert multiline string to JSON-safe format
          ESCAPED_PROMPT=$(echo "$REVIEW_PROMPT" | jq -Rs .)

          # Make the API call to Devin
          RESPONSE=$(curl -s -X POST \
            -H "Authorization: Bearer $DEVIN_API_KEY" \
            -H "Content-Type: application/json" \
            -d "{\"prompt\": $ESCAPED_PROMPT}" \
            "https://api.devin.ai/v1/sessions")

          # Debug: Print the raw response
          echo "Raw Devin Response:"
          echo "$RESPONSE"

          # Extract session details
          SESSION_ID=$(echo "$RESPONSE" | jq -r '.session_id // empty')
          if [ -z "$SESSION_ID" ] || [ "$SESSION_ID" = "null" ]; then
            echo "Error: Failed to get valid session ID from response"
            echo "Response was: $RESPONSE"
            exit 1
          fi

          echo "session-id=$SESSION_ID" >> $GITHUB_OUTPUT